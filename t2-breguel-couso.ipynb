{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/17 23:47:40 WARN Utils: Your hostname, MacBook-Pro-de-Sebastian-1028.local resolves to a loopback address: 127.0.0.1; using 192.168.1.200 instead (on interface en0)\n",
      "24/06/17 23:47:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/17 23:47:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.200:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciar BBD Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j\n",
    "from neo4j import GraphDatabase\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Neo4j established successfully!\n"
     ]
    }
   ],
   "source": [
    "URI = \"neo4j+s://a3d230a8.databases.neo4j.io\"\n",
    "AUTH = (\"neo4j\", \"PDOH3crDmCKi-BEnScXmynkqDkuc2bZGmv5WgHgG6lQ\")\n",
    "\n",
    "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "with driver.session() as session:\n",
    "    try:\n",
    "        session.run(\"RETURN 1\")\n",
    "        print(\"Connection to Neo4j established successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Neo4j: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(tx, graph_data):\n",
    "    # Create nodes\n",
    "    nodes = set()\n",
    "    for start_node, relationship_type, end_node in graph_data:\n",
    "        nodes.add(start_node)\n",
    "        nodes.add(end_node)\n",
    "\n",
    "    for node in nodes:\n",
    "        tx.run(\"MERGE (n:Node {id: $id})\", id=node)\n",
    "\n",
    "    # Create relationships\n",
    "    for start_node, relationship_type, end_node in graph_data:\n",
    "        tx.run(\n",
    "            \"\"\"\n",
    "            MATCH (a:Node {id: $start_id})\n",
    "            MATCH (b:Node {id: $end_id})\n",
    "            MERGE (a)-[r:RELATIONSHIP {type: $type}]->(b)\n",
    "            \"\"\",\n",
    "            start_id=start_node,\n",
    "            end_id=end_node,\n",
    "            type=relationship_type,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example graph data\n",
    "\n",
    "graph_data = [\n",
    "    (1, 11, 2),\n",
    "    (1, 11, 3),\n",
    "    (2, 11, 3),\n",
    "    (3, 11, 2),\n",
    "    (3, 11, 4),\n",
    "    (4, 11, 1),\n",
    "    (4, 11, 2),\n",
    "    (4, 11, 3),\n",
    "    (4, 12, 5),\n",
    "    (5, 12, 1),\n",
    "    (5, 12, 2),\n",
    "    (5, 12, 6),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph instantiated successfully!\n"
     ]
    }
   ],
   "source": [
    "with driver.session() as session:\n",
    "    try:\n",
    "        session.execute_write(create_graph, graph_data)\n",
    "        print(\"Graph instantiated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to instantiate the graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 1\n",
    "\n",
    "Implementa una función que reciba un grafo en Neo4j y genere una RDD con las aristas de ese grafo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Edge = namedtuple(\"Edge\", [\"n1\", \"R\", \"n2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Edge(n1=31349, R='cites', n2=31336),\n",
       " Edge(n1=686532, R='cites', n2=31336),\n",
       " Edge(n1=1129442, R='cites', n2=31336),\n",
       " Edge(n1=1107312, R='cites', n2=13195),\n",
       " Edge(n1=1120731, R='cites', n2=13195),\n",
       " Edge(n1=755217, R='cites', n2=13195),\n",
       " Edge(n1=1105116, R='cites', n2=37879),\n",
       " Edge(n1=686532, R='cites', n2=31349),\n",
       " Edge(n1=137849, R='cites', n2=109323),\n",
       " Edge(n1=154134, R='cites', n2=217139),\n",
       " Edge(n1=31336, R='cites', n2=31353),\n",
       " Edge(n1=31349, R='cites', n2=31353),\n",
       " Edge(n1=1152272, R='cites', n2=31353),\n",
       " Edge(n1=1124844, R='cites', n2=31353),\n",
       " Edge(n1=1135746, R='cites', n2=31353)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    MATCH (a:Paper)-[r:CITES]->(b:Paper)\n",
    "    RETURN a.id AS start_node, b.id AS end_node\n",
    "    \"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    edges = [\n",
    "        Edge(record[\"start_node\"], \"cites\", record[\"end_node\"]) for record in result\n",
    "    ]\n",
    "\n",
    "graph_rdd = sc.parallelize(edges)\n",
    "\n",
    "graph_rdd.collect()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 2\n",
    "\n",
    "* Implementa un programa en PySpark que entregue todos los triángulos (como tuplas de tres nodos) en el grafo usando b3 reducers, donde b es un parámetro. Para esta primera parte puedes asumir que tu grafo solo usa una etiqueta de arista (en el grafo de prueba, esa etiqueta corresponde al numero 11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase de Map\n",
    "\n",
    "* Generamos pares de (llave, valor) para las aristas del grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map procedure\n",
    "def hash_node(node_id: int, b: int) -> int:\n",
    "    return hash(node_id) % b\n",
    "\n",
    "\n",
    "def generate_n_keys(edge, b, n):\n",
    "    hashed_n1 = hash_node(edge.n1, b)\n",
    "    hashed_n2 = hash_node(edge.n2, b)\n",
    "\n",
    "    combinations = itertools.product(range(b), repeat=n - 2)\n",
    "\n",
    "    return (((hashed_n1, hashed_n2) + comb, edge) for comb in combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shifts(key):\n",
    "    n = len(key)\n",
    "    gen = [tuple(key[i:] + key[:i]) for i in range(n)]\n",
    "    return gen\n",
    "\n",
    "\n",
    "def map_to_shifts(key_edge_tuple):\n",
    "    key, edge_value = key_edge_tuple\n",
    "\n",
    "    return [((shifted_key), edge_value) for shifted_key in generate_shifts(key)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase de Reduce\n",
    "\n",
    "* Agrupamos llaves según si coinciden en términos de un shift de carrusel: (b1, b2, b3) = (b2, b3, b1) = (b3, b1, b2)\n",
    "* Detectamos ciclos del tamaño deseado para cada reducer mediante DFS y retornamos los nodos asociados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(edges):\n",
    "    graph = {}\n",
    "\n",
    "    for edge in edges:\n",
    "        start = edge.n1\n",
    "        end = edge.n2\n",
    "        if start not in graph:\n",
    "            graph[start] = []\n",
    "        graph[start].append(end)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def dfs_iterative(graph, start_node, n):\n",
    "    stack = [(start_node, 0, [start_node])]\n",
    "    visited_paths = set()\n",
    "\n",
    "    while stack:\n",
    "        node, path_length, path = stack.pop()\n",
    "\n",
    "        if path_length == n:\n",
    "            if node == start_node:\n",
    "                return True, tuple(set(path))\n",
    "            continue\n",
    "\n",
    "        if node in graph:\n",
    "            for neighbor in graph[node]:\n",
    "                new_path = path + [neighbor]\n",
    "                new_path_tuple = (node, neighbor)\n",
    "\n",
    "                # Here we add memory to dont have repeated paths, so 2 -> 3, 3 -> 2, is a cycle of length 2, not 2n\n",
    "                if new_path_tuple not in visited_paths:\n",
    "                    visited_paths.add((node, neighbor))\n",
    "                    stack.append((neighbor, path_length + 1, new_path))\n",
    "                    visited_paths.add(new_path_tuple)\n",
    "\n",
    "    return False, ()\n",
    "\n",
    "\n",
    "def has_cycle_of_length_n(edges, n):\n",
    "    graph = create_graph(edges)\n",
    "    cycles = set()\n",
    "    found = False\n",
    "\n",
    "    for node in graph:\n",
    "        cycle_detected, cycle_nodes = dfs_iterative(graph, node, n)\n",
    "\n",
    "        if cycle_detected:\n",
    "            found = True\n",
    "            cycles.add(cycle_nodes)\n",
    "                       \n",
    "    if found:\n",
    "        return True, list(cycles)\n",
    "    else:\n",
    "        return False, [()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle of length 4 detected: [(1, 2, 3, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "edges = [\n",
    "    Edge(1, \"cites\", 2),\n",
    "    Edge(2, \"cites\", 3),\n",
    "    Edge(3, \"cites\", 4),\n",
    "    Edge(4, \"cites\", 1),\n",
    "    Edge(2, \"cites\", 4),\n",
    "    Edge(4, \"cites\", 5),\n",
    "]\n",
    "n = 4  # Length of the cycle we want to detect\n",
    "\n",
    "\n",
    "cycle_detected, cycle_nodes = has_cycle_of_length_n(edges, n)\n",
    "\n",
    "if cycle_detected:\n",
    "    print(f\"Cycle of length {n} detected: {cycle_nodes}\")\n",
    "else:\n",
    "    print(f\"No cycle of length {n} found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_uni_relation(graph, b, n):\n",
    "\n",
    "    #1: Map to the keys\n",
    "    mapped_graph_rdd = graph.flatMap(lambda edge: generate_n_keys(edge, b, n))\n",
    "\n",
    "    #2: Shift the keys\n",
    "    aggregated_rdd = mapped_graph_rdd.flatMap(map_to_shifts).groupByKey().mapValues(list)\n",
    "\n",
    "    #3: Detect cycles\n",
    "    cycles_rdd = aggregated_rdd.map(\n",
    "        lambda key_edge_pair: (key_edge_pair[0], has_cycle_of_length_n(key_edge_pair[1], n))\n",
    "    )\n",
    "\n",
    "    #4: Filter the cycles\n",
    "    filtered_rdd = cycles_rdd.filter(lambda x: x[1][0])\n",
    "\n",
    "    #5: Extract nodes\n",
    "    nodes_rdd = filtered_rdd.flatMap(lambda x: [(x[0], path) for path in x[1][1]])\n",
    "\n",
    "    #6: Sort the nodes\n",
    "    nodes_rdd2 = nodes_rdd.map(lambda x: (x[0], tuple(sorted(list(x[1])))))\n",
    "\n",
    "    #7: Remove duplicates\n",
    "    queried_triangles = nodes_rdd2.values().distinct()\n",
    "\n",
    "    return queried_triangles\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30,\n",
       " [(753047, 753070, 753264),\n",
       "  (3191, 3192, 5086),\n",
       "  (12195, 12350, 51180),\n",
       "  (1997, 3233, 49811),\n",
       "  (9586, 33818, 78557),\n",
       "  (60159, 60169, 399370),\n",
       "  (35, 35061, 210871),\n",
       "  (648106, 648112, 648121),\n",
       "  (16819, 643221, 643239),\n",
       "  (23502, 184918, 330148),\n",
       "  (642894, 643221, 643485),\n",
       "  (103543, 126920, 126927),\n",
       "  (12576, 56112, 83725),\n",
       "  (5064, 5069, 28026),\n",
       "  (12195, 38722, 51180),\n",
       "  (6898, 12631, 12638),\n",
       "  (12350, 51180, 67415),\n",
       "  (35, 210871, 273152),\n",
       "  (153063, 561568, 561613),\n",
       "  (642894, 643221, 643239),\n",
       "  (33818, 78552, 78557),\n",
       "  (34263, 34266, 87482),\n",
       "  (2695, 2698, 342802),\n",
       "  (643221, 643485, 644577),\n",
       "  (31932, 194617, 215912),\n",
       "  (20178, 64271, 91852),\n",
       "  (119761, 143801, 284025),\n",
       "  (9586, 33818, 78552),\n",
       "  (31769, 67245, 67246),\n",
       "  (126920, 126927, 645897)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 7\n",
    "n = 3\n",
    "\n",
    "result = pipeline_uni_relation(graph_rdd, b, n).collect()\n",
    "len(result), result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 3\n",
    "\n",
    "* Asume ahora que recibes un subgrafo como tres arreglos: un arreglo A con las variables, otro L con los tipos de aristas, y una matriz M de tamaño |A| × |L| × |A| que tiene un uno en la posicion (x, R, y) si y solo si (x, R, y) es una arista de tu subgrafo. \n",
    "* Implementa un programa en PySpark que reciba un patrón que tiene solo variables, y exactamente cuatro variables, y entregue todos los matches de ese patrón (como tuplas de 4 nodos) en el grafo usando b4 reducers, donde b es un parámetro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "Para este caso el Map se hara solamente para los casos las combinaciones donde las relaciones son especificadas en el patrón.\n",
    "\n",
    "Con esto reducimos el espacio de búsqueda y incorporamos las relaciones en el proceso de búsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, pensando en que `n1` puede hacer match con `x` y `n2` con `y`, el mapper debe generar llaves `(h(n1), h(n2), 0)`, ..., `(h(n1), h(n2), b - 1)`, y emitir los mensajes que corresponden a esa llave con el valor `(n1, r, n2)`.\n",
    "\n",
    "Pensando en que `n1` puede hacer match con `y` y `n2` con `z`, el mapper debe generar llaves `(0, h(n1), h(n2))`, ..., `(b - 1, h(n1), h(n2))`, y emitir los mensajes que corresponden a esa llave con el valor `(n1, r, n2)`.\n",
    "\n",
    "Finalmente, pensando en que `n1` puede hacer match con `z` y `n2` con `x`, el mapper debe generar llaves `(h(n2), 0, h(n1))`, ..., `(h(n2), b - 1, h(n1))`, y emitir los mensajes que corresponden a esa llave con el valor `(n1, r, n2)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map procedure\n",
    "def hash_node(node_id: int, b: int) -> int:\n",
    "    return hash(node_id) % b\n",
    "\n",
    "\n",
    "\n",
    "def generate_keys(edge, b, rel_order):\n",
    "    hashed_n1 = hash_node(edge.n1, b)\n",
    "    hashed_n2 = hash_node(edge.n2, b)\n",
    "    edge_rel = edge.R\n",
    "\n",
    "    keys = []\n",
    "    n = len(rel_order)\n",
    "    combinations = itertools.product(range(b), repeat=n - 2)\n",
    "\n",
    "    # Generar llaves según el orden de las relaciones\n",
    "    for comb in combinations:\n",
    "        if edge_rel == rel_order[0]:\n",
    "            keys.append(((hashed_n1, hashed_n2) + comb, edge))\n",
    "\n",
    "        if edge_rel == rel_order[1]:\n",
    "            keys.append(((comb[0], hashed_n1, hashed_n2) + comb[1:], edge))\n",
    "\n",
    "        if n == 3 and edge_rel == rel_order[2]:\n",
    "            keys.append(((hashed_n2, comb[0], hashed_n1) + comb[1:], edge))\n",
    "\n",
    "        if n == 4:\n",
    "            if edge_rel == rel_order[2]:\n",
    "                keys.append(((comb[0], comb[1], hashed_n1, hashed_n2), edge))\n",
    "                \n",
    "            if edge_rel == rel_order[3]:\n",
    "                keys.append(((hashed_n2, comb[0], comb[1], hashed_n1), edge))\n",
    "\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt DFS\n",
    "\n",
    "Para este caso, lo que hicimos fue adaptar el anterior algoritmo de DFS, para asi incoporar las relaciones y que ademas de buscar, tambien asegurarnos el grafo sigue cierto orden sequencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(edges):\n",
    "    graph = {}\n",
    "\n",
    "    for edge in edges:\n",
    "        start = edge.n1\n",
    "        end = edge.n2\n",
    "        relation = edge.R\n",
    "        if start not in graph:\n",
    "            graph[start] = []\n",
    "        graph[start].append((end, relation))\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def dfs__relation_iterative(graph, start_node, n, rel_order):\n",
    "    stack = [(start_node, 0, [start_node], [])]\n",
    "    visited_paths = set()\n",
    "\n",
    "    while stack:\n",
    "        node, path_length, path, rels = stack.pop()\n",
    "\n",
    "        if path_length == n:\n",
    "            if node == start_node and rels == rel_order:\n",
    "                return True, tuple(set(path))\n",
    "            continue\n",
    "\n",
    "        if node in graph:\n",
    "            for neighbor, relation in graph[node]:\n",
    "                new_path = path + [neighbor]\n",
    "                new_rels = rels + [relation]\n",
    "                new_path_tuple = (node, relation, neighbor)\n",
    "\n",
    "                # Here we add memory to dont have repeated paths, so 2 -> 3, 3 -> 2, is a cycle of length 2, not 2n\n",
    "                if new_path_tuple not in visited_paths:\n",
    "                    visited_paths.add(new_path_tuple)\n",
    "                    stack.append((neighbor, path_length + 1, new_path, new_rels))\n",
    "                    \n",
    "    return False, ()\n",
    "\n",
    "\n",
    "def has_cycle_of_length_n_with_order(edges, n, rel_order):\n",
    "    graph = create_graph(edges)\n",
    "    cycles = set()\n",
    "    found = False\n",
    "\n",
    "    for node in graph:\n",
    "        cycle_detected, cycle_nodes = dfs__relation_iterative(graph, node, n, rel_order)\n",
    "\n",
    "        if cycle_detected:\n",
    "            found = True\n",
    "            cycles.add(cycle_nodes)\n",
    "                       \n",
    "    if found:\n",
    "        return True, list(cycles)\n",
    "    else:\n",
    "        return False, [()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline de Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_relation(graph, relations, b):\n",
    "\n",
    "    n = len(relations)  # Length of the cycle we want to detect\n",
    "    \n",
    "    #1: Map to the keys\n",
    "    mapped_graph_rdd = graph.flatMap(lambda edge: generate_keys(edge, b, relations))\n",
    "\n",
    "    #2: group by keys\n",
    "    aggregated_rdd = mapped_graph_rdd.groupByKey().mapValues(list)\n",
    "\n",
    "    #3: Detect cycles\n",
    "    cycles_rdd = aggregated_rdd.map(\n",
    "        lambda key_edge_pair: (\n",
    "            key_edge_pair[0],\n",
    "            has_cycle_of_length_n_with_order(key_edge_pair[1], n, relations),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #4: Filter the cycles\n",
    "    filtered_rdd = cycles_rdd.filter(lambda x: x[1][0])\n",
    "\n",
    "    #5: Extract nodes\n",
    "    nodes_rdd = filtered_rdd.flatMap(lambda x: [(x[0], path) for path in x[1][1]])\n",
    "\n",
    "    #6: Delete duplicates\n",
    "    nodes_rdd2 = nodes_rdd.map(lambda x: (tuple(sorted(x[1])), ( x[1])))\n",
    "    grouped_by_neighbors = nodes_rdd2.groupByKey()\n",
    "    distinct_nodes = grouped_by_neighbors.map(lambda x: (list(x[1])[0]))\n",
    "\n",
    "    return distinct_nodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results, relations):\n",
    "    for nodes in results:\n",
    "            output = \"\"\n",
    "            for i in range(len(nodes)):\n",
    "            # Agrega cada relación a la cadena de salida\n",
    "                output += f\"({nodes[i]} -> {relations[i]} > {nodes[(i + 1) % len(nodes)]}), \"\n",
    "\n",
    "            # Imprime la cadena de salida sin la última coma y espacio\n",
    "            print(output[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grafo de papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " [(33818, 78557, 9586, 78511),\n",
       "  (12195, 51180, 12350, 67415),\n",
       "  (16819, 643221, 642894, 643239),\n",
       "  (87482, 34266, 34263, 90655),\n",
       "  (12210, 51180, 12350, 67415),\n",
       "  (38722, 51180, 12350, 67415),\n",
       "  (78552, 9586, 33818, 78557),\n",
       "  (78552, 9586, 33818, 78511),\n",
       "  (38722, 12195, 51180, 12350),\n",
       "  (35, 35061, 141342, 210871)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 10  # Number of buckets\n",
    "R_1 = \"cites\"\n",
    "R_2 = \"cites\"\n",
    "R_3 = \"cites\"\n",
    "R_4 = \"cites\"\n",
    "relations = [R_1, R_2, R_3, R_4]\n",
    "\n",
    "results = pipeline_relation(graph_rdd, relations, b).collect()\n",
    "len(results), results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph example\n",
    "\n",
    "Ejemplo del grafo de clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Edge(n1=1, R=11, n2=2),\n",
       " Edge(n1=1, R=11, n2=3),\n",
       " Edge(n1=2, R=11, n2=3),\n",
       " Edge(n1=3, R=11, n2=2),\n",
       " Edge(n1=3, R=11, n2=4),\n",
       " Edge(n1=4, R=11, n2=1),\n",
       " Edge(n1=4, R=11, n2=2),\n",
       " Edge(n1=4, R=11, n2=3),\n",
       " Edge(n1=4, R=12, n2=5),\n",
       " Edge(n1=5, R=12, n2=1),\n",
       " Edge(n1=5, R=12, n2=2),\n",
       " Edge(n1=5, R=12, n2=6)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    Edge(1, 11, 2),\n",
    "    Edge(1, 11, 3),\n",
    "    Edge(2, 11, 3),\n",
    "    Edge(3, 11, 2),\n",
    "    Edge(3, 11, 4),\n",
    "    Edge(4, 11, 1),\n",
    "    Edge(4, 11, 2),\n",
    "    Edge(4, 11, 3),\n",
    "    Edge(4, 12, 5),\n",
    "    Edge(5, 12, 1),\n",
    "    Edge(5, 12, 2),\n",
    "    Edge(5, 12, 6),\n",
    "]\n",
    "edge_rdd = sc.parallelize(data)\n",
    "\n",
    "edge_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, [(2, 3, 4, 5), (1, 3, 4, 5)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 10  # Number of buckets\n",
    "R_1 = 11\n",
    "R_2 = 11\n",
    "R_3 = 12\n",
    "R_4 = 12\n",
    "relations = [R_1, R_2, R_3, R_4]\n",
    "\n",
    "result = pipeline_relation(edge_rdd, relations, b).collect()\n",
    "len(result), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2 -> 11 > 3), (3 -> 11 > 4), (4 -> 12 > 5), (5 -> 12 > 2)\n",
      "(1 -> 11 > 3), (3 -> 11 > 4), (4 -> 12 > 5), (5 -> 12 > 1)\n"
     ]
    }
   ],
   "source": [
    "print_results(result, relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafíos/Problemas a resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buckets(b): 2 - b^4: 1.60e+01, Number of cycles: 9, Time: 0.88 seconds\n",
      "Number of buckets(b): 3 - b^4: 8.10e+01, Number of cycles: 10, Time: 0.48 seconds\n",
      "Number of buckets(b): 5 - b^4: 6.25e+02, Number of cycles: 8, Time: 0.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buckets(b): 10 - b^4: 1.00e+04, Number of cycles: 10, Time: 1.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buckets(b): 20 - b^4: 1.60e+05, Number of cycles: 10, Time: 3.86 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of buckets(b): 30 - b^4: 8.10e+05, Number of cycles: 10, Time: 13.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "R_1 = \"cites\"\n",
    "R_2 = \"cites\"\n",
    "R_3 = \"cites\"\n",
    "R_4 = \"cites\"\n",
    "relations = [R_1, R_2, R_3, R_4]\n",
    "\n",
    "for b in [2, 3, 5, 10, 20, 30]:\n",
    "    before = time.time()\n",
    "    result = pipeline_relation(graph_rdd, relations, b).collect()\n",
    "    after = time.time()\n",
    "    print(f\"Number of buckets(b): {b:,} - b^4: {b**4:.2e}, Number of cycles: {len(result):,}, Time: {after - before:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datos-masivos-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
